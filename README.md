Adversarial attack is a technique used to manipulate machine learning models by intentionally inputting maliciously crafted data in order to deceive the model's output. The goal of an adversarial attack is to cause the model to make incorrect predictions or classifications. Hence, an adversarial example is an input to a machine learning model that is purposely designed to cause a model to make a mistake in its predictions despite resembling a valid input to a human.

In this project ResNet50 model was trained to distinguish penguins from turtles and then adversarial attack with fast gradient sign method was performed, so penguin was classified as turtle.
